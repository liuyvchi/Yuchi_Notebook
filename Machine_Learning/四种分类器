CART, Bagging, Random Forest, Boosting

from：http://blog.csdn.net/abcjennifer/article/details/8164315

CART（Classification And Regression Tree）
         Breiman, Friedman, Olshen & Stone (1984), Quinlan (1993)
         思想：递归地将输入空间分割成矩形
         优点：可以进行变量选择，可以克服missing data，可以处理混合预测
         缺点：不稳定

Bagging (Breiman1996): 也称bootstrap aggregation
Bagging的策略：
         - 从样本集中用Bootstrap采样选出n个样本
         - 在所有属性上，对这n个样本建立分类器（CART or SVM or ...）
         - 重复以上两步m次，i.e.build m个分类器（CART or SVM or ...）
         - 将数据放在这m个分类器上跑，最后vote看到底分到哪一类
         
Random forest(Breiman1999):
随机森林在bagging基础上做了修改。
         - 从样本集中用Bootstrap采样选出n个样本，预建立CART
         - 在树的每个节点上，从所有属性中随机选择k个属性，选择出一个最佳分割属性作为节点
         - 重复以上两步m次，i.e.build m棵CART
         - 这m个CART形成Random Forest

随机森林可以既可以处理属性为离散值的量，比如ID3算法，也可以处理属性为连续值的量，比如C4.5算法。
这里的random就是指
         1. Bootstrap中的随机选择子样本   
         2. Random subspace的算法从属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的
         
Boosting(Freund & Schapire 1996):
Fit many large or small trees to reweighted versions of the training data. Classify by weighted majority vote.
首先给个大致的概念，boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本（i.e.分错类的样本weight大）。
怎么做的呢？
         - boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。

数据挖掘的十大算法，以后可以慢慢研究：
C4.5
K-Means
SVM
Apriori
EM
PageRank
AdaBoost
kNN
NaiveBayes
CART
